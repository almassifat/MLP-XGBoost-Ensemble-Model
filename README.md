# MLP-XGBoost-Ensemble-Model
Compare two products using BERT, MLP, and XGBoost ensemble 
This project implements a hybrid deep learning and gradient boosting ensemble model to compare two products and determine which one is better â€” based on specifications, reviews, and pricing. It combines BERT-based embeddings with traditional machine learning to balance interpretability and accuracy.

ğŸ“¦ Features
ğŸ§  BERT-based review embedding (DistilBERT/BERT-base)

ğŸ”¢ Tabular features (CPU, GPU, RAM, SSD, Rating, Price)

âš™ï¸ Deep learning (MLP) model + XGBoost ensemble



ğŸ“ˆ Performance visualizations and evaluation metrics

ğŸ§¾ Project Overview
Step	Description
1ï¸âƒ£	Data analysis, cleaning, normalization

2ï¸âƒ£	Feature engineering (difference-based)

3ï¸âƒ£	Text encoding with HuggingFace BERT

4ï¸âƒ£	Deep learning MLP model

5ï¸âƒ£	XGBoost classifier

6ï¸âƒ£	Ensemble using probability averaging

Evaluation: accuracy, precision, recall, F1, AUC, curves


ğŸ“Š Example Output

Confusion Matrix

Accuracy vs Loss Curve

# ğŸ§  Model Highlights
Uses BERT to capture semantic meaning from reviews

Custom importance weights for CPU/GPU over display

Ensemble boosts performance beyond individual models

# How To Run:

Clone the repo:

git clone https://github.com/yourusername/MLP-XGBoost-Ensemble-Model.git

cd MLP-XGBoost-Ensemble-Model

Install dependencies:

pip install -r requirements.txt

Open and run the notebook in Colab or locally using Jupyter:

model_training.ipynb

Upload the dataset from Google Drive or update the path in the notebook.


